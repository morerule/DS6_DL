{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morerule/DS6_DL/blob/main/%EA%BD%83%EB%B6%84%EB%A5%98_VGG_%EC%A0%84%EC%9D%B4_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8_%EC%B6%9C%EB%A0%A5%EC%82%AD%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PP09m2w58i4Q"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
        "    name='tf_flowers',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    download=True,\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")"
      ],
      "metadata": {
        "id": "qJYNO5Ik8tDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 160 # 리사이징할 이미지의 크기\n",
        "\n",
        "def format_example(image, label):\n",
        "    image = tf.cast(image, tf.float32)  # image=float(image)같은 타입캐스팅의  텐서플로우 버전입니다.\n",
        "    image = (image/127.5) - 1 # 픽셀값의 scale 수정\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    return image, label\n",
        "\n",
        "train = raw_train.map(format_example)\n",
        "validation = raw_validation.map(format_example)\n",
        "test = raw_test.map(format_example)"
      ],
      "metadata": {
        "id": "LCpVtZJD8ueF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "\n",
        "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "validation_batches = validation.batch(BATCH_SIZE)\n",
        "test_batches = test.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "I8H7StIW8_Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "# Create the base model from the pre-trained model VGG16\n",
        "base_model = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\n",
        "                                         include_top=False,\n",
        "                                         weights='imagenet')"
      ],
      "metadata": {
        "id": "HCMZjmC183sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image_batch, label_batch in train_batches.take(1):\n",
        "    break\n",
        "\n",
        "image_batch.shape, label_batch.shape"
      ],
      "metadata": {
        "id": "kp4-A6-i9ta1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_batch = base_model(image_batch)\n",
        "feature_batch.shape"
      ],
      "metadata": {
        "id": "riAg1atO99Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"
      ],
      "metadata": {
        "id": "r0auECLx9-EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_batch_average = global_average_layer(feature_batch)\n",
        "print(feature_batch_average.shape)"
      ],
      "metadata": {
        "id": "UDCK4Fhu-KlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dense layer에는 unit과 activation 2개의 매개변수만 사용해주세요.\n",
        "# unit의 값은 위에서 global_average_layer를 통과했을 때의 값을 생각해보세요.\n",
        "# 활성화 함수는 ReLU를 사용합니다.\n",
        "dense_layer = tf.keras.layers.Dense(units=256,\n",
        "                                    activation='relu')\n",
        "# unit은 우리가 분류하고 싶은 class를 생각해보세요.\n",
        "# 활성화 함수는 Softmax를 사용합니다.\n",
        "prediction_layer = tf.keras.layers.Dense(units=5,\n",
        "                                         activation='softmax')\n",
        "# feature_batch_averag가 dense_layer를 거친 결과가 다시 prediction_layer를 거치게 되면\n",
        "prediction_batch = prediction_layer(dense_layer(feature_batch_average))\n",
        "print(prediction_batch.shape)"
      ],
      "metadata": {
        "id": "7QqZzPi7-Sic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable = False"
      ],
      "metadata": {
        "id": "VnCtuOqx_DL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  global_average_layer,\n",
        "  dense_layer,\n",
        "  prediction_layer\n",
        "])"
      ],
      "metadata": {
        "id": "1142ELFb_EyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "seb_cRx-_N0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_learning_rate = 0.0001\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "GMJCVklS_O0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_steps=20\n",
        "loss0, accuracy0 = model.evaluate(validation_batches, steps = validation_steps)\n",
        "\n",
        "print(\"initial loss: {:.2f}\".format(loss0))\n",
        "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
      ],
      "metadata": {
        "id": "OuxBVyBr_UFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(train_batches,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=validation_batches)"
      ],
      "metadata": {
        "id": "Si3IN21lA-H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5QM9nJUbA_7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image_batch, label_batch in test_batches.take(1):\n",
        "    images = image_batch\n",
        "    labels = label_batch\n",
        "    predictions = model.predict(image_batch)\n",
        "    pass\n",
        "\n",
        "predictions"
      ],
      "metadata": {
        "id": "K4L4x93dBBRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "predictions"
      ],
      "metadata": {
        "id": "F4pFpJqIBCQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 12))\n",
        "\n",
        "for idx, (image, label, prediction) in enumerate(zip(images, labels, predictions)):\n",
        "    plt.subplot(4, 8, idx+1)\n",
        "    image = (image + 1) / 2\n",
        "    plt.imshow(image)\n",
        "    correct = label == prediction\n",
        "    title = f'real: {label} / pred :{prediction}\\n {correct}!'\n",
        "    if not correct:\n",
        "        plt.title(title, fontdict={'color': 'red'})\n",
        "    else:\n",
        "        plt.title(title, fontdict={'color': 'blue'})\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "C27l0OiVBCrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for image, label, prediction in zip(images, labels, predictions):\n",
        "    correct = label == prediction\n",
        "    if correct:\n",
        "        count = count + 1\n",
        "\n",
        "print(count / 32 * 100) # 약 95% 내외"
      ],
      "metadata": {
        "id": "9KClBFhNBD0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "에폭을 늘리는것 만으로 87.5 나옴 더 높은 점수를 위해 파인튜닝 실행"
      ],
      "metadata": {
        "id": "8iP0CBluIOjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [1] 라이브러리 불러오기\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# [2] 데이터셋 로드 (tf_flowers)\n",
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
        "    name='tf_flowers',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    download=True,\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "\n",
        "# [3] 데이터 전처리 함수\n",
        "IMG_SIZE = 160\n",
        "def format_example(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image / 127.5) - 1\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    return image, label\n",
        "\n",
        "train = raw_train.map(format_example)\n",
        "validation = raw_validation.map(format_example)\n",
        "test = raw_test.map(format_example)\n",
        "\n",
        "# [4] 배치 구성\n",
        "BATCH_SIZE = 32\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "validation_batches = validation.batch(BATCH_SIZE)\n",
        "test_batches = test.batch(BATCH_SIZE)\n",
        "\n",
        "# [5] VGG16 기반 모델 불러오기\n",
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "base_model = tf.keras.applications.VGG16(\n",
        "    input_shape=IMG_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# [6] Global Average Pooling + Dense 레이어\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "dense_layer = tf.keras.layers.Dense(units=256, activation='relu')\n",
        "prediction_layer = tf.keras.layers.Dense(units=5, activation='softmax')\n",
        "\n",
        "# [7] 모델 구성\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    global_average_layer,\n",
        "    dense_layer,\n",
        "    prediction_layer\n",
        "])\n",
        "\n",
        "# [8] 전이학습 단계\n",
        "base_model.trainable = False\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# [9] 초기 평가\n",
        "validation_steps = 20\n",
        "loss0, accuracy0 = model.evaluate(validation_batches, steps=validation_steps)\n",
        "print(\"initial loss: {:.2f}\".format(loss0))\n",
        "print(\"initial accuracy: {:.2f}\".format(accuracy0))\n",
        "\n",
        "# [10] 학습\n",
        "EPOCHS = 15\n",
        "history = model.fit(\n",
        "    train_batches,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_batches\n",
        ")\n",
        "\n",
        "# [11] 학습 곡선 시각화\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Transfer Learning Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Transfer Learning Loss')\n",
        "plt.show()\n",
        "\n",
        "# [12] Fine-tuning 단계 추가\n",
        "base_model.trainable = True\n",
        "fine_tune_at = 10\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Fine-tuning은 기존 가중치를 크게 흔들지 않도록 학습률을 낮게 설정\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# [13] Fine-tuning 학습\n",
        "EPOCHS_FINE = 5\n",
        "history_fine = model.fit(\n",
        "    train_batches,\n",
        "    epochs=EPOCHS_FINE,\n",
        "    validation_data=validation_batches\n",
        ")\n",
        "\n",
        "# [14] Fine-tuning 학습 곡선 시각화\n",
        "acc = history_fine.history['accuracy']\n",
        "val_acc = history_fine.history['val_accuracy']\n",
        "loss = history_fine.history['loss']\n",
        "val_loss = history_fine.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Fine-tuning Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Fine-tuning Loss')\n",
        "plt.show()\n",
        "\n",
        "# [15] Test 데이터 평가\n",
        "test_loss, test_acc = model.evaluate(test_batches)\n",
        "print(\"Test accuracy after fine-tuning: {:.2f}\".format(test_acc))\n",
        "\n",
        "# [16] 테스트셋 일부 예측 시각화\n",
        "for image_batch, label_batch in test_batches.take(1):\n",
        "    images = image_batch\n",
        "    labels = label_batch\n",
        "    predictions = model.predict(image_batch)\n",
        "\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "plt.figure(figsize=(20, 12))\n",
        "for idx, (image, label, prediction) in enumerate(zip(images, labels.numpy(), predictions)):\n",
        "    plt.subplot(4, 8, idx+1)\n",
        "    image = (image + 1) / 2\n",
        "    plt.imshow(image)\n",
        "    correct = label == prediction\n",
        "    title = f'real: {label} / pred: {prediction}\\n {correct} !'\n",
        "    if not correct:\n",
        "        plt.title(title, fontdict={'color': 'red'})\n",
        "    else:\n",
        "        plt.title(title, fontdict={'color': 'blue'})\n",
        "    plt.axis('off')\n",
        "\n",
        "# [17] 배치 정확도 계산\n",
        "labels_np = labels.numpy()\n",
        "accuracy = np.mean(labels_np == predictions) * 100\n",
        "print(\"Batch accuracy: {:.2f}%\".format(accuracy))\n"
      ],
      "metadata": {
        "id": "ZFDJiwVYlEzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "파인튜닝을 했더니 정확도가 오르긴 했지만 과적합 발생하여 데이터 증강 레이어를 추가"
      ],
      "metadata": {
        "id": "H9E6FKGf6sPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [1] 라이브러리 불러오기\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# [2] 데이터셋 로드 (tf_flowers)\n",
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
        "    name='tf_flowers',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    download=True,\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "\n",
        "# [3] 데이터 전처리 함수\n",
        "IMG_SIZE = 160\n",
        "def format_example(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image / 127.5) - 1\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    return image, label\n",
        "\n",
        "train = raw_train.map(format_example)\n",
        "validation = raw_validation.map(format_example)\n",
        "test = raw_test.map(format_example)\n",
        "\n",
        "# [4] 배치 구성\n",
        "BATCH_SIZE = 32\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "validation_batches = validation.batch(BATCH_SIZE)\n",
        "test_batches = test.batch(BATCH_SIZE)\n",
        "\n",
        "# [5] VGG16 기반 모델 불러오기\n",
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "base_model = tf.keras.applications.VGG16(\n",
        "    input_shape=IMG_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# [6] Global Average Pooling + Dense 레이어\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "dense_layer = tf.keras.layers.Dense(units=256, activation='relu')\n",
        "prediction_layer = tf.keras.layers.Dense(units=5, activation='softmax')\n",
        "\n",
        "# 데이터 증강 레이어 추가\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip('horizontal'),\n",
        "  tf.keras.layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "# [7] 모델 구성\n",
        "model = tf.keras.Sequential([\n",
        "    data_augmentation,\n",
        "    base_model,\n",
        "    global_average_layer,\n",
        "    dense_layer,\n",
        "    prediction_layer\n",
        "])\n",
        "\n",
        "# [8] 전이학습 단계\n",
        "base_model.trainable = False\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# [9] 초기 평가\n",
        "validation_steps = 20\n",
        "loss0, accuracy0 = model.evaluate(validation_batches, steps=validation_steps)\n",
        "print(\"initial loss: {:.2f}\".format(loss0))\n",
        "print(\"initial accuracy: {:.2f}\".format(accuracy0))\n",
        "\n",
        "# [10] 학습 (Transfer Learning)\n",
        "EPOCHS = 20\n",
        "history = model.fit(\n",
        "    train_batches,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_batches\n",
        ")\n",
        "\n",
        "# [11] 학습 곡선 시각화\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Transfer Learning Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Transfer Learning Loss')\n",
        "plt.show()\n",
        "\n",
        "# [12] Fine-tuning 단계 추가\n",
        "base_model.trainable = True\n",
        "fine_tune_at = 10\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# [13] Fine-tuning 학습\n",
        "EPOCHS_FINE = 10\n",
        "history_fine = model.fit(\n",
        "    train_batches,\n",
        "    epochs=EPOCHS_FINE,\n",
        "    validation_data=validation_batches\n",
        ")\n",
        "\n",
        "# [14] Fine-tuning 학습 곡선 시각화\n",
        "acc = history_fine.history['accuracy']\n",
        "val_acc = history_fine.history['val_accuracy']\n",
        "loss = history_fine.history['loss']\n",
        "val_loss = history_fine.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Fine-tuning Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Fine-tuning Loss')\n",
        "plt.show()\n",
        "\n",
        "# [15] Test 데이터 평가\n",
        "test_loss, test_acc = model.evaluate(test_batches)\n",
        "print(\"Test accuracy after fine-tuning: {:.2f}\".format(test_acc))\n",
        "\n",
        "# [16] 테스트셋 일부 예측 시각화\n",
        "for image_batch, label_batch in test_batches.take(1):\n",
        "    images = image_batch\n",
        "    labels = label_batch\n",
        "    predictions = model.predict(image_batch)\n",
        "\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "plt.figure(figsize=(20, 12))\n",
        "for idx, (image, label, prediction) in enumerate(zip(images, labels.numpy(), predictions)):\n",
        "    plt.subplot(4, 8, idx+1)\n",
        "    image = (image + 1) / 2\n",
        "    plt.imshow(image)\n",
        "    correct = label == prediction\n",
        "    title = f'real: {label} / pred: {prediction}\\n {correct} !'\n",
        "    if not correct:\n",
        "        plt.title(title, fontdict={'color': 'red'})\n",
        "    else:\n",
        "        plt.title(title, fontdict={'color': 'blue'})\n",
        "    plt.axis('off')\n",
        "\n",
        "# [17] 배치 정확도 계산\n",
        "labels_np = labels.numpy()\n",
        "accuracy = np.mean(labels_np == predictions) * 100\n",
        "print(\"Batch accuracy: {:.2f}%\".format(accuracy))\n"
      ],
      "metadata": {
        "id": "dBUl09Eu6nAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 증강을 하고 에폭을 기존그대로 했더니 정확도는 차이없이 93.5가 나왔으나 학습은 좋았음 그래서 에폭을 늘렸더니 오히려 정확도가 87.5로 감소함 이번엔 드롭아웃 추가"
      ],
      "metadata": {
        "id": "PJNup3M7-bU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [1] 라이브러리 불러오기\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# [2] 데이터셋 로드 (tf_flowers)\n",
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
        "    name='tf_flowers',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    download=True,\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "\n",
        "# [3] 데이터 전처리 함수\n",
        "IMG_SIZE = 160\n",
        "def format_example(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image / 127.5) - 1\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    return image, label\n",
        "\n",
        "train = raw_train.map(format_example)\n",
        "validation = raw_validation.map(format_example)\n",
        "test = raw_test.map(format_example)\n",
        "\n",
        "# [4] 배치 구성\n",
        "BATCH_SIZE = 32\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "validation_batches = validation.batch(BATCH_SIZE)\n",
        "test_batches = test.batch(BATCH_SIZE)\n",
        "\n",
        "# [5] VGG16 기반 모델 불러오기\n",
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "base_model = tf.keras.applications.VGG16(\n",
        "    input_shape=IMG_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# [6] Global Average Pooling + Dense 레이어\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "dense_layer = tf.keras.layers.Dense(units=256, activation='relu')\n",
        "prediction_layer = tf.keras.layers.Dense(units=5, activation='softmax')\n",
        "\n",
        "# 데이터 증강 레이어 추가\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip('horizontal'),\n",
        "  tf.keras.layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "# [7] 모델 구성\n",
        "model = tf.keras.Sequential([\n",
        "    data_augmentation,\n",
        "    base_model,\n",
        "    global_average_layer,\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    dense_layer,\n",
        "    prediction_layer\n",
        "])\n",
        "\n",
        "# [8] 전이학습 단계\n",
        "base_model.trainable = False\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# [9] 초기 평가\n",
        "validation_steps = 20\n",
        "loss0, accuracy0 = model.evaluate(validation_batches, steps=validation_steps)\n",
        "print(\"initial loss: {:.2f}\".format(loss0))\n",
        "print(\"initial accuracy: {:.2f}\".format(accuracy0))\n",
        "\n",
        "# [10] 학습 (Transfer Learning)\n",
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    train_batches,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_batches\n",
        ")\n",
        "\n",
        "# [11] 학습 곡선 시각화\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Transfer Learning Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Transfer Learning Loss')\n",
        "plt.show()\n",
        "\n",
        "# [12] Fine-tuning 단계 추가\n",
        "base_model.trainable = True\n",
        "fine_tune_at = 10\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# [13] Fine-tuning 학습\n",
        "EPOCHS_FINE = 20\n",
        "history_fine = model.fit(\n",
        "    train_batches,\n",
        "    epochs=EPOCHS_FINE,\n",
        "    validation_data=validation_batches\n",
        ")\n",
        "\n",
        "# [14] Fine-tuning 학습 곡선 시각화\n",
        "acc = history_fine.history['accuracy']\n",
        "val_acc = history_fine.history['val_accuracy']\n",
        "loss = history_fine.history['loss']\n",
        "val_loss = history_fine.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Fine-tuning Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Fine-tuning Loss')\n",
        "plt.show()\n",
        "\n",
        "# [15] Test 데이터 평가\n",
        "test_loss, test_acc = model.evaluate(test_batches)\n",
        "print(\"Test accuracy after fine-tuning: {:.2f}\".format(test_acc))\n",
        "\n",
        "# [16] 테스트셋 일부 예측 시각화\n",
        "for image_batch, label_batch in test_batches.take(1):\n",
        "    images = image_batch\n",
        "    labels = label_batch\n",
        "    predictions = model.predict(image_batch)\n",
        "\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "plt.figure(figsize=(20, 12))\n",
        "for idx, (image, label, prediction) in enumerate(zip(images, labels.numpy(), predictions)):\n",
        "    plt.subplot(4, 8, idx+1)\n",
        "    image = (image + 1) / 2\n",
        "    plt.imshow(image)\n",
        "    correct = label == prediction\n",
        "    title = f'real: {label} / pred: {prediction}\\n {correct} !'\n",
        "    if not correct:\n",
        "        plt.title(title, fontdict={'color': 'red'})\n",
        "    else:\n",
        "        plt.title(title, fontdict={'color': 'blue'})\n",
        "    plt.axis('off')\n",
        "\n",
        "# [17] 배치 정확도 계산\n",
        "labels_np = labels.numpy()\n",
        "accuracy = np.mean(labels_np == predictions) * 100\n",
        "print(\"Batch accuracy: {:.2f}%\".format(accuracy))\n"
      ],
      "metadata": {
        "id": "OBghr9Ns-8L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "기본 에폭15 에서는 학습이 잘안되는데 파인튜닝 에폭10 에서는 학습이 잘됨 결과는 93.75 기본 에폭은10으로 줄이고 파인튜닝 에폭을 20으로늘려 재학습 해봤으나 파인튜닝 에폭은 10이상에서 효과없음"
      ],
      "metadata": {
        "id": "dHwuKLgOAhAb"
      }
    }
  ]
}